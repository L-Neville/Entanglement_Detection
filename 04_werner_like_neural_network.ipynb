{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qiskit.quantum_info import random_clifford, Pauli\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def tensor_prod(*tensors):\n",
    "    if len(tensors) == 2:\n",
    "        return np.kron(tensors[0], tensors[1])\n",
    "    else:\n",
    "        return np.kron(tensors[0], tensor_prod(*tensors[1:]))\n",
    "    \n",
    "def hermitian(matrix):\n",
    "    return np.allclose(matrix, matrix.conj().T)\n",
    "\n",
    "def trace_one(matrix):\n",
    "    return np.isclose(np.trace(matrix), 1)\n",
    "\n",
    "def positive_semi_definite(matrix, tol=1e-8):\n",
    "    return np.all(np.linalg.eigvals(matrix) + tol >= 0)\n",
    "\n",
    "def is_legal(matrix):\n",
    "    return hermitian(matrix) and trace_one(matrix) and positive_semi_definite(matrix)\n",
    "\n",
    "def int_to_bin_list(n, length):\n",
    "    bin_list = np.zeros(length)\n",
    "    bin_list[n] = 1\n",
    "    return bin_list\n",
    "\n",
    "def single_sample(prob_list):\n",
    "    assert np.isclose(sum(prob_list), 1), \"probability does not sum up to 1\"\n",
    "    rd = np.random.random()\n",
    "    inf, sup = 0, 0\n",
    "    for i, e in enumerate(prob_list):\n",
    "        sup += e\n",
    "        if inf <= rd <= sup:\n",
    "            return i\n",
    "        else:\n",
    "            inf = sup\n",
    "    raise ValueError(\"random value does not meet any interval\")\n",
    "\n",
    "class QuantumState():\n",
    "    def __init__(self, num_qubits:int, num_shots:int, batch_size:int, pauli_observables:list, veri:bool):\n",
    "        self._num_qubits = num_qubits\n",
    "        self._observables = pauli_observables\n",
    "        self._batch_size = batch_size\n",
    "        self._num_shots = num_shots\n",
    "        self._veri = veri\n",
    "        self._dm = None\n",
    "        self._entangled = None\n",
    "        \n",
    "    @property\n",
    "    def dm(self):\n",
    "        return self._dm\n",
    "    \n",
    "    @dm.setter\n",
    "    def dm(self, new_dm):\n",
    "        if not (self._veri or is_legal(new_dm)):\n",
    "            raise ValueError(\"density matrix is not physical\")\n",
    "        else:\n",
    "            self._dm = new_dm\n",
    "    \n",
    "    def set_dm(self):\n",
    "        raise NotImplementedError(\"without information to construct density matrix\")\n",
    "    \n",
    "    def random_evolve(self):\n",
    "        self._U = random_clifford(self._num_qubits).to_matrix()\n",
    "        self._dm = self._U @ self.dm @ np.conj(self._U).T\n",
    "    \n",
    "    def single_shot_measure(self):\n",
    "        prob_list = [self._dm[i, i] for i in range(2 ** self._num_qubits)]\n",
    "        single_shot_state = int_to_bin_list(single_sample(prob_list), 2 ** self._num_qubits)\n",
    "        del self._dm\n",
    "        self._state = single_shot_state\n",
    "    \n",
    "    def reconstruct_dm(self):\n",
    "        dim = 2 ** self._num_qubits\n",
    "        return (dim + 1) * (np.conj(self._U).T @ np.outer(self._state, self._state) @ self._U) - np.eye(dim)\n",
    "\n",
    "    # def classical_shadow(self):\n",
    "    #     shadows = {obs: [] for obs in self._observables}\n",
    "    #     temp_shadows = {obs: [] for obs in self._observables}\n",
    "    #     dm_copy = self._dm\n",
    "    #     for _ in range(self._num_shots // self._batch_size):\n",
    "    #         for _ in range(self._batch_size):\n",
    "    #             self._dm = dm_copy\n",
    "    #             self.random_evolve()\n",
    "    #             self.single_shot_measure()\n",
    "    #             rdm = self.reconstruct_dm()\n",
    "    #             for k, v in temp_shadows.items():\n",
    "    #                 v.append(np.trace(Pauli(k).to_matrix() @ rdm))\n",
    "    #         for k, v in shadows.items():\n",
    "    #             v.append(np.mean(temp_shadows[k]))\n",
    "    #         temp_shadows = {obs: [] for obs in self._observables}\n",
    "    #     del temp_shadows\n",
    "    #     return {k: np.median(v) for k, v in shadows.items()}\n",
    "    \n",
    "    def classical_shadow(self):\n",
    "        shadows = {obs: [] for obs in self._observables}\n",
    "        dm_copy = self._dm\n",
    "        for _ in range(self._num_shots // self._batch_size):\n",
    "            snapshots = []\n",
    "            for _ in range(self._batch_size):\n",
    "                self._dm = dm_copy\n",
    "                self.random_evolve()\n",
    "                self.single_shot_measure()\n",
    "                snapshots.append(self.reconstruct_dm())\n",
    "            mean = np.mean(np.stack(snapshots), axis=0)\n",
    "            for k, v in shadows.items():\n",
    "                v.append(np.trace(Pauli(k).to_matrix() @ mean))\n",
    "        return {k: np.median(v) for k, v in shadows.items()}\n",
    "    \n",
    "def partial_transpose(matrix):\n",
    "    if matrix.shape != (4, 4):\n",
    "        raise ValueError(\"Input matrix must be 4x4.\")\n",
    "    result = np.zeros((4, 4), dtype=matrix.dtype)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            for k in range(2):\n",
    "                for l in range(2):\n",
    "                    result[2*i + k, 2*j + l] = matrix[2*i + k, 2*j + l]\n",
    "                    result[2*i + k, 2*j + l] = matrix[2*i + l, 2*j + k]\n",
    "    return result\n",
    "\n",
    "state_01 = np.array([[0], \n",
    "                     [1],\n",
    "                     [0],\n",
    "                     [0]])\n",
    "state_10 = np.array([[0], \n",
    "                     [0],\n",
    "                     [1],\n",
    "                     [0]])\n",
    "\n",
    "class Wernerlikestate(QuantumState):\n",
    "    def __init__(self, p, theta, phi, num_qubits:int, num_shots:int, batch_size:int, pauli_observables:list, veri:bool):\n",
    "        super().__init__(num_qubits, num_shots, batch_size, pauli_observables, veri)\n",
    "        assert num_qubits == 2, \"Werner-like states contain only 2 qubits\"\n",
    "        self._p = p\n",
    "        self._theta = theta\n",
    "        self._phi = phi\n",
    "        assert 0 <= p <= 1, \"Werner-like state parameter F must lie between 0 and 1\"\n",
    "\n",
    "    def set_dm(self):\n",
    "        psi = np.cos(self._theta) * state_01 + np.exp(1j * self._phi) * np.sin(self._theta) * state_10\n",
    "        # if self._p == 1:\n",
    "        #     if is_legal(tensor_prod(psi, np.conj(psi).T)):\n",
    "        #         self._dm = tensor_prod(psi, np.conj(psi).T)\n",
    "        #         return self._dm\n",
    "        #     else:\n",
    "        #         raise ValueError(\"wtf\")\n",
    "        new_dm = self._p * tensor_prod(psi, np.conj(psi).T) + (1 - self._p) * np.eye(4) / 4\n",
    "        if is_legal(new_dm):\n",
    "            self._dm = new_dm\n",
    "            return self._dm\n",
    "        else:\n",
    "            print(f\"error occur: p={self._p}, theta={self._theta}, phi={self._phi}\")\n",
    "            raise NotImplementedError(\"density matrix setting wrongly implemented\")\n",
    "    \n",
    "    @property\n",
    "    def entangled(self):\n",
    "        pt_dm = partial_transpose(self._dm)\n",
    "        return not positive_semi_definite(pt_dm)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.14611562383912968, Accuracy: 0.9407125\n",
      "Epoch 2/20, Loss: 0.057984001808586254, Accuracy: 0.9785\n",
      "Epoch 3/20, Loss: 0.03750318008487645, Accuracy: 0.986375\n",
      "Epoch 4/20, Loss: 0.02980573901885889, Accuracy: 0.989175\n",
      "Epoch 5/20, Loss: 0.02543276343469983, Accuracy: 0.990925\n",
      "Epoch 6/20, Loss: 0.023652451820313034, Accuracy: 0.991425\n",
      "Epoch 7/20, Loss: 0.021907621225286947, Accuracy: 0.992425\n",
      "Epoch 8/20, Loss: 0.020840951325328105, Accuracy: 0.99255\n",
      "Epoch 9/20, Loss: 0.020863209101794088, Accuracy: 0.9927125\n",
      "Epoch 10/20, Loss: 0.01951467954165043, Accuracy: 0.993275\n",
      "Epoch 11/20, Loss: 0.019181493763035795, Accuracy: 0.993225\n",
      "Epoch 12/20, Loss: 0.019133749609896836, Accuracy: 0.9931625\n",
      "Epoch 13/20, Loss: 0.017488397163184653, Accuracy: 0.9938875\n",
      "Epoch 14/20, Loss: 0.017395299430117184, Accuracy: 0.9937625\n",
      "Epoch 15/20, Loss: 0.01658000994606581, Accuracy: 0.994175\n",
      "Epoch 16/20, Loss: 0.016337970184160604, Accuracy: 0.9938625\n",
      "Epoch 17/20, Loss: 0.016574209746873323, Accuracy: 0.9943375\n",
      "Epoch 18/20, Loss: 0.016134229483068783, Accuracy: 0.99415\n",
      "Epoch 19/20, Loss: 0.015589298595468344, Accuracy: 0.9945\n",
      "Epoch 20/20, Loss: 0.016637322285055305, Accuracy: 0.9940375\n",
      "Validation Loss: 0.006530216320416073, Validation Accuracy: 0.99735\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"output\\werner_like_data.csv\")\n",
    "\n",
    "# Extract features and target\n",
    "X = dataset[['XY', 'ZZ', 'ZI']].values\n",
    "y = dataset['result'].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y).unsqueeze(1).float()  # Add an extra dimension for the labels\n",
    "\n",
    "# Create a dataset and split into training and validation sets\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "class ImprovedBinaryClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedBinaryClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = ImprovedBinaryClassificationModel()\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize the weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = (outputs > 0.5).float()  # Convert probabilities to binary predictions\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Print statistics\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}')\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_correct_predictions = 0\n",
    "val_total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        # Calculate validation accuracy\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        val_correct_predictions += (predicted == labels).sum().item()\n",
    "        val_total_samples += labels.size(0)\n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "val_accuracy = val_correct_predictions / val_total_samples\n",
    "print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
