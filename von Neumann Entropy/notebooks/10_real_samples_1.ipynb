{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit, transpile, assemble\n",
    "from qiskit.quantum_info import Statevector, DensityMatrix, Pauli, random_clifford, random_statevector, partial_trace\n",
    "from qiskit_aer import AerSimulator, Aer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import sqrtm, logm\n",
    "import cvxpy as cp\n",
    "import pickle\n",
    "import itertools, functools, collections, warnings, copy, winsound, os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def tensor_prod(*tensors):\n",
    "    if len(tensors) == 2:\n",
    "        return np.kron(tensors[0], tensors[1])\n",
    "    else:\n",
    "        return np.kron(tensors[0], tensor_prod(*tensors[1:]))\n",
    "    \n",
    "def hermitian(matrix):\n",
    "    return np.allclose(matrix, matrix.conj().T)\n",
    "\n",
    "def trace_one(matrix):\n",
    "    return np.isclose(np.trace(matrix), 1)\n",
    "\n",
    "def positive_semi_definite(matrix, tol=1e-5):\n",
    "    return np.all(np.linalg.eigvalsh(matrix) + tol >= 0)\n",
    "\n",
    "def is_legal(matrix):\n",
    "    return hermitian(matrix) and trace_one(matrix) and positive_semi_definite(matrix)\n",
    "\n",
    "def check_legal(matrix, print_msg=True):\n",
    "    errors, legal = [], True\n",
    "    if not hermitian(matrix):\n",
    "        errors.append('not hermitian')\n",
    "    if not trace_one(matrix):\n",
    "        errors.append('trace not equal to one')\n",
    "    if not positive_semi_definite(matrix):\n",
    "        errors.append('not positive semidefinite')\n",
    "    legal = (len(errors) == 0)\n",
    "    if not legal:\n",
    "        msg = f'input is not legal: ' + '; '.join(errors)\n",
    "    else: \n",
    "        msg = 'input is a legal density matrix'\n",
    "    if print_msg:\n",
    "        print(msg)\n",
    "    return legal, msg\n",
    "        \n",
    "        \n",
    "def generate_prob_lst(num_states):\n",
    "    prob_lst = np.array([np.random.random() for _ in range(num_states)])\n",
    "    prob_lst /= np.sum(prob_lst)\n",
    "    return prob_lst\n",
    "\n",
    "def get_rank(dm, tol=1e-10):\n",
    "    return int(np.sum(np.linalg.eigvalsh(dm) > tol))\n",
    "\n",
    "def get_fidelity(dm1, dm2, tol=1e-5):\n",
    "    # assert is_legal(dm1) and is_legal(dm2), 'inputs are not legal density matrices'\n",
    "    if not is_legal(dm1):\n",
    "        warnings.warn(\"input dm1 is not a legal density matrix\", UserWarning)\n",
    "    if not is_legal(dm2):\n",
    "        warnings.warn(\"input dm2 is not a legal density matrix\", UserWarning)        \n",
    "    try: \n",
    "        fidelity = (np.trace(sqrtm(sqrtm(dm1) @ dm2 @ sqrtm(dm1)))) ** 2\n",
    "    except ValueError:\n",
    "        print('fidelity cannot be computed for the given inputs')\n",
    "    # assert np.abs(np.imag(fidelity)) < tol, 'fidelity is not real within tol'\n",
    "    if np.abs(np.imag(fidelity)) > tol:\n",
    "        warnings.warn(f\"the fidelity has an imaginary part larger than tol: {np.abs(np.imag(fidelity))}\")\n",
    "    return fidelity.real\n",
    "\n",
    "def get_purity(dm, tol=1e-5):\n",
    "    if not is_legal(dm):\n",
    "        warnings.warn(\"input is not a legal density matrix\", UserWarning)\n",
    "    try:\n",
    "        purity = np.trace(dm @ dm)\n",
    "    except ValueError:\n",
    "        print('purity cannot be computed for the given inputs')\n",
    "    # assert np.abs(np.imag(purity)) < tol, 'purity is not real within tol'\n",
    "    if np.abs(np.imag(purity)) > tol:\n",
    "        warnings.warn(f\"the purity has an imaginary part larger than tol: {np.abs(np.imag(purity))}\")\n",
    "    return purity.real        \n",
    "\n",
    "def generate_dm(num_qubits, num_states, state_lst=None, prob_lst=None, prime_prob=None):\n",
    "    assert (prob_lst is None) or (prime_prob is None), 'cannot set prob_lst and prime_prob together'\n",
    "    if state_lst is None:\n",
    "        state_lst = [random_statevector(2**num_qubits) for _ in range(num_states)]\n",
    "    if prime_prob is not None:\n",
    "        prob_lst = np.array([prime_prob] + (generate_prob_lst(num_states - 1) * (1 - prime_prob)).tolist())\n",
    "    elif prob_lst is None:\n",
    "        prob_lst = generate_prob_lst(num_states)\n",
    "    density_matrix = sum([DensityMatrix(state_lst[i]).data * prob_lst[i] for i in range(num_states)])\n",
    "    return density_matrix\n",
    "\n",
    "def single_sample(prob_list):\n",
    "    assert np.isclose(sum(prob_list), 1), \"probability does not sum up to 1\"\n",
    "    def alias_setup(probabilities):\n",
    "        n = len(probabilities)\n",
    "        prob = [0] * n\n",
    "        alias = [0] * n\n",
    "        scaled_prob = [p * n for p in probabilities]\n",
    "        small = []\n",
    "        large = []\n",
    "        for i, sp in enumerate(scaled_prob):\n",
    "            if sp < 1.0:\n",
    "                small.append(i)\n",
    "            else:\n",
    "                large.append(i)\n",
    "        while small and large:\n",
    "            small_index = small.pop()\n",
    "            large_index = large.pop()\n",
    "            prob[small_index] = scaled_prob[small_index]\n",
    "            alias[small_index] = large_index\n",
    "            scaled_prob[large_index] = (scaled_prob[large_index] + scaled_prob[small_index]) - 1.0\n",
    "            if scaled_prob[large_index] < 1.0:\n",
    "                small.append(large_index)\n",
    "            else:\n",
    "                large.append(large_index)\n",
    "        for i in large:\n",
    "            prob[i] = 1.0\n",
    "        for i in small:\n",
    "            prob[i] = 1.0\n",
    "        return prob, alias\n",
    "    def alias_sample(prob, alias):\n",
    "        n = len(prob)\n",
    "        i = np.random.randint(0, n)\n",
    "        r = np.random.random()\n",
    "        if r < prob[i]:\n",
    "            return i\n",
    "        else:\n",
    "            return alias[i]\n",
    "    return alias_sample(*alias_setup(prob_list))\n",
    "\n",
    "\n",
    "\n",
    "def sample_from_dict(d, n_samples):\n",
    "    def alias_setup(probs):\n",
    "        n = len(probs)\n",
    "        alias = np.zeros(n, dtype=int)\n",
    "        prob = np.zeros(n, dtype=np.float64)\n",
    "        scaled_probs = np.array(probs) * n\n",
    "        small = []\n",
    "        large = []\n",
    "        for i, sp in enumerate(scaled_probs):\n",
    "            if sp < 1.0:\n",
    "                small.append(i)\n",
    "            else:\n",
    "                large.append(i)\n",
    "        while small and large:\n",
    "            small_idx = small.pop()\n",
    "            large_idx = large.pop()\n",
    "\n",
    "            prob[small_idx] = scaled_probs[small_idx]\n",
    "            alias[small_idx] = large_idx\n",
    "\n",
    "            scaled_probs[large_idx] = scaled_probs[large_idx] + scaled_probs[small_idx] - 1.0\n",
    "\n",
    "            if scaled_probs[large_idx] < 1.0:\n",
    "                small.append(large_idx)\n",
    "            else:\n",
    "                large.append(large_idx)\n",
    "        while large:\n",
    "            large_idx = large.pop()\n",
    "            prob[large_idx] = 1.0\n",
    "        while small:\n",
    "            small_idx = small.pop()\n",
    "            prob[small_idx] = 1.0\n",
    "        return alias, prob\n",
    "    def alias_draw(alias, prob):\n",
    "        n = len(alias)\n",
    "        i = np.random.randint(n)\n",
    "        if np.random.rand() < prob[i]:\n",
    "            return i\n",
    "        else:\n",
    "            return alias[i]\n",
    "    keys = list(d.keys())\n",
    "    probs = list(d.values())\n",
    "    alias, prob = alias_setup(probs)\n",
    "    samples = [keys[alias_draw(alias, prob)] for _ in range(n_samples)]\n",
    "    return samples\n",
    "\n",
    "def expand_to_tensor_product(array):\n",
    "    index = np.argmax(array)\n",
    "    n = int(np.log2(len(array)))\n",
    "    binary_string = format(index, f'0{n}b')\n",
    "    tensor_product = []\n",
    "    for bit in binary_string:\n",
    "        if bit == '0':\n",
    "            tensor_product.append(np.array([1, 0]))\n",
    "        else:\n",
    "            tensor_product.append(np.array([0, 1]))\n",
    "    return tensor_product\n",
    "\n",
    "def int_to_bin_list(n, length):\n",
    "    bin_list = np.zeros(length)\n",
    "    bin_list[n] = 1\n",
    "    return bin_list\n",
    "\n",
    "def split_and_calculate_mean(values, group_size):\n",
    "    groups = [values[i:i + group_size] for i in range(0, len(values), group_size)]\n",
    "    means = [np.sum(group, axis=0) / len(group) for group in groups]\n",
    "    return means\n",
    "\n",
    "def generate_random_01_strings(num_strings, length):\n",
    "    characters = ['0', '1']\n",
    "    generated_strings = []\n",
    "    assert num_strings < 2 ** length, 'too much strings to generate'\n",
    "    for _ in range(num_strings):\n",
    "        while True:\n",
    "            random_string = ''.join(np.random.choice(characters) for _ in range(length))\n",
    "            if random_string != '0' * length and random_string not in generated_strings:\n",
    "                generated_strings.append(random_string)\n",
    "                break\n",
    "    return generated_strings\n",
    "\n",
    "def generate_uv_Pauli_matrix(u_vec, v_vec):\n",
    "    uv_map = {\n",
    "        '00':'I', '01':'Z', '10':'X', '11':'Y'\n",
    "    }\n",
    "    Pauli_string = ''\n",
    "    for u, v in zip(u_vec, v_vec):\n",
    "        Pauli_char = uv_map.get(str(u) + str(v))\n",
    "        if Pauli_char is None:\n",
    "            raise ValueError('u or v list contains elements neither 0 or 1')\n",
    "        else:\n",
    "            Pauli_string += Pauli_char\n",
    "    return Pauli_string\n",
    "\n",
    "def generate_all_binary(length):\n",
    "    all_strings = itertools.product('01', repeat=length)\n",
    "    result = [''.join(s) for s in all_strings if '1' in s]\n",
    "    return result\n",
    "\n",
    "def generate_Pauli_expectations(dm, obsv):\n",
    "    return np.trace(dm @ Pauli(obsv).to_matrix()).real\n",
    "\n",
    "def get_trace_norm(dm):\n",
    "    return np.sum(np.linalg.svd(dm, compute_uv=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_states = {'0': np.array([[1], [0]]), '1': np.array([[0], [1]])}\n",
    "\n",
    "class State():\n",
    "    def __init__(self, strings):\n",
    "        if len(strings) == 1:\n",
    "            self.state = single_states[strings]\n",
    "        else:\n",
    "            singles = [single_states[s] for s in strings]\n",
    "            self.state = tensor_prod(*singles)\n",
    "    def to_vector(self):\n",
    "        return self.state\n",
    "\n",
    "def generate_random_Pauli_strings(num_strings, num_qubits, pattern='balanced'):\n",
    "    assert pattern in ['balanced', 'pro_I', 'pro_XYZ', 'uv_pair', 'only_XYZ'], 'please choose pattern from: balanced, pro_I, pro_XYZ, uv_pair, only_XYZ'\n",
    "    generated_strings = []\n",
    "    characters = ['X', 'Y', 'Z', 'I']\n",
    "    if pattern == 'only_XYZ':\n",
    "        assert 0 < num_strings <= 3 ** num_qubits, 'too much or too few strings to generate'\n",
    "        return np.random.choice([''.join(obsv) for obsv in list(itertools.product(*['XYZ' for _ in range(num_qubits)]))], num_strings, replace=False)\n",
    "    if pattern == 'balanced':\n",
    "        assert 0 < num_strings <= 4 ** num_qubits - 1, 'too much or too few strings to generate'\n",
    "        for _ in range(num_strings):\n",
    "            while True:\n",
    "                random_string = ''.join(np.random.choice(characters) for _ in range(num_qubits))\n",
    "                if random_string != 'I' * num_qubits and random_string not in generated_strings:\n",
    "                    generated_strings.append(random_string)\n",
    "                    break\n",
    "        return generated_strings\n",
    "    if pattern == 'uv_pair':\n",
    "        assert 0 < num_strings <= 4 ** num_qubits - 1, 'too much or too few strings to generate'\n",
    "        uv_map = {'00':'I', '01':'Z', '10':'X', '11':'Y'}\n",
    "        whole, remain = num_strings // 2 ** (num_qubits), num_strings % 2 ** (num_qubits)\n",
    "        if whole > 0:\n",
    "            v_lst = [format(i, f'0{num_qubits}b') for i in range(2 ** num_qubits)]\n",
    "            u_lst = np.random.choice([format(i, f'0{num_qubits}b') for i in range(1, 2 ** num_qubits)], whole, replace=False).tolist()\n",
    "            for u, v in list(itertools.product(u_lst, v_lst)):\n",
    "                generated_strings.append(''.join([uv_map[u_char + v_char] for u_char, v_char in zip(u, v)]))\n",
    "        if remain > 0:\n",
    "            u_lst = ['0' * num_qubits]\n",
    "            v_lst = np.random.choice([format(i, f'0{num_qubits}b') for i in range(1, 2 ** num_qubits)], remain, replace=False).tolist()\n",
    "            for u, v in list(itertools.product(u_lst, v_lst)):\n",
    "                generated_strings.append(''.join([uv_map[u_char + v_char] for u_char, v_char in zip(u, v)]))\n",
    "        return generated_strings\n",
    "    all_strings = generate_random_Pauli_strings(4 ** num_qubits - 1, num_qubits, pattern='balanced')\n",
    "    grouped = dict()\n",
    "    for i in range(num_qubits):\n",
    "        grouped[i] = []\n",
    "    for string in all_strings:\n",
    "        grouped[string.count('I')].append(string)\n",
    "    if pattern == 'pro_I':\n",
    "        assert 0 < num_strings <= 4 ** num_qubits - 1, 'too much or too few strings to generate'\n",
    "        i = num_qubits - 1\n",
    "        while len(generated_strings) < num_strings:\n",
    "            if num_strings - len(generated_strings) >= len(grouped[i]):\n",
    "                generated_strings += grouped[i]\n",
    "            else:\n",
    "                generated_strings += np.random.choice(grouped[i], num_strings - len(generated_strings), replace=True).tolist()\n",
    "            i -= 1\n",
    "        return generated_strings\n",
    "    if pattern == 'pro_XYZ':\n",
    "        assert 0 < num_strings <= 4 ** num_qubits - 1, 'too much or too few strings to generate'\n",
    "        i = 0\n",
    "        while len(generated_strings) < num_strings:\n",
    "            if num_strings - len(generated_strings) >= len(grouped[i]):\n",
    "                generated_strings += grouped[i]\n",
    "            else:\n",
    "                generated_strings += np.random.choice(grouped[i], num_strings - len(generated_strings), replace=True).tolist()\n",
    "            i += 1\n",
    "        return generated_strings\n",
    "    \n",
    "def broadcast_string(s):\n",
    "    def helper(s, index, current, results):\n",
    "        if index == len(s):\n",
    "            results.add(current)\n",
    "            return\n",
    "        # Option 1: Keep the current character\n",
    "        helper(s, index + 1, current + s[index], results)\n",
    "        # Option 2: Replace the current character with 'I'\n",
    "        helper(s, index + 1, current + 'I', results)\n",
    "\n",
    "    results = set()\n",
    "    helper(s, 0, '', results)\n",
    "    return results\n",
    "\n",
    "def broadcast_all_strings(strings):\n",
    "    all_broadcasts = set()\n",
    "    for s in strings:\n",
    "        all_broadcasts.update(broadcast_string(s))\n",
    "    return all_broadcasts\n",
    "    \n",
    "def estimate_Pauli_expectations(dm, obsv, num_samples, simulation=False):\n",
    "    num_samples = int(num_samples)\n",
    "    if simulation: # simulate the process of sampling\n",
    "        exp = np.real(np.trace(dm @ Pauli(obsv).to_matrix()))\n",
    "        prob_p1 = (1 + exp) / 2\n",
    "        prob_m1 = 1 - prob_p1\n",
    "        samples = np.random.choice([+1, -1], size=num_samples, p=[prob_p1, prob_m1])\n",
    "        return np.mean(samples)\n",
    "    else: # use the approximate distribution instead\n",
    "        exp = np.real(np.trace(dm @ Pauli(obsv).to_matrix()))\n",
    "        num_samples_root = num_samples ** .5\n",
    "        std_dev = (1 - exp ** 2) ** .5 / num_samples_root\n",
    "        return np.random.normal(exp, std_dev)\n",
    "    \n",
    "def broadcast_Pauli_expectations(dm, num_qubits, obserables, num_samples):\n",
    "    repetition = num_samples // len(obserables)\n",
    "    original = set(obserables)\n",
    "    broadcasted = broadcast_all_strings(obserables).difference(original)\n",
    "    expectations = {key: [] for key in broadcasted.union(original)}\n",
    "    remaining = copy.deepcopy(broadcasted)\n",
    "    converter = {\n",
    "        'X': np.array([[1, 1], [1, -1]]) / np.sqrt(2), \n",
    "        'Y': np.array([[1, 0], [0, 1j]], dtype=np.complex128) @ (np.array([[1, 1], [1, -1]]) / np.sqrt(2)), \n",
    "        'Z': np.array([[1, 0], [0, 1]])\n",
    "    }\n",
    "    all_states = [''.join(state) for state in list(itertools.product(*['01' for _ in range(num_qubits)]))]\n",
    "    for obsv in original:\n",
    "        overall_converter = tensor_prod(*[converter[s] for s in obsv])\n",
    "        converted_dm = overall_converter.conj().T @ dm @ overall_converter\n",
    "        distributions = {state: (State(state).to_vector().conj().T @ converted_dm @ State(state).to_vector())[0][0].real for state in all_states}\n",
    "        samples = dict(collections.Counter(sample_from_dict(distributions, repetition)))\n",
    "        probabilities = [samples[state] / repetition if state in samples.keys() else 0 for state in all_states]\n",
    "        indices = range(len(obsv))\n",
    "        parities = [(-1) ** (sum([1 for i in indices if state[i] == '1'])) for state in all_states]\n",
    "        expectations[obsv] = sum([prob * parity for prob, parity in zip(probabilities, parities)])\n",
    "        if len(remaining) > 0:\n",
    "            for obs in broadcast_all_strings([obsv]).difference(obsv):\n",
    "                if obs in remaining:\n",
    "                    indices = [i for i in range(len(obs)) if not obs[i] == 'I']\n",
    "                    parities = [(-1) ** (sum([1 for i in indices if state[i] == '1'])) for state in all_states]\n",
    "                    expectations[obs] = sum([prob * parity for prob, parity in zip(probabilities, parities)])\n",
    "                    remaining.remove(obs)\n",
    "    return expectations\n",
    "            \n",
    "    \n",
    "def get_partial_trace(dm, subsystems):\n",
    "    return partial_trace(dm, subsystems).data\n",
    "\n",
    "def get_von_neumann_entropy(dm, r=None):\n",
    "    # if not np.allclose(dm, dm.conj().T):\n",
    "    #     raise ValueError(\"The density matrix must be Hermitian.\")\n",
    "    if r is None:\n",
    "        eigenvalues = np.linalg.eigvalsh(dm)\n",
    "        eigenvalues = eigenvalues[eigenvalues > 0]\n",
    "        entropy = - np.sum(eigenvalues * np.log(eigenvalues))\n",
    "    else:\n",
    "        eigenvalues = np.linalg.eigvalsh(dm)\n",
    "        eigenvalues = np.partition(eigenvalues, -r)[-r:]\n",
    "        entropy = - np.sum(eigenvalues * np.log(eigenvalues))\n",
    "    return entropy\n",
    "\n",
    "def get_TMI(dm, r=None):\n",
    "    dm_s = get_partial_trace(dm, [1, 2])\n",
    "    dm_m1 = get_partial_trace(dm, [0, 2])\n",
    "    dm_m2 = get_partial_trace(dm, [0, 1])\n",
    "    dm_m = get_partial_trace(dm, [0])\n",
    "    dm_sm1 = get_partial_trace(dm, [2])\n",
    "    dm_sm2 = get_partial_trace(dm, [1])\n",
    "    i2_sm1 = get_von_neumann_entropy(dm_s, r) + get_von_neumann_entropy(dm_m1, r) - get_von_neumann_entropy(dm_sm1, r)\n",
    "    i2_sm2 = get_von_neumann_entropy(dm_s, r) + get_von_neumann_entropy(dm_m2, r) - get_von_neumann_entropy(dm_sm2, r)\n",
    "    i2_sm = get_von_neumann_entropy(dm_s, r) + get_von_neumann_entropy(dm_m, r) - get_von_neumann_entropy(dm, r)\n",
    "    return i2_sm1 + i2_sm2 - i2_sm\n",
    "\n",
    "def regularize(dm):\n",
    "    dm /= np.trace(dm)\n",
    "    dm = (dm + dm.conj().T) / 2\n",
    "    return dm\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumState():\n",
    "    def __init__(self, num_qubits:int, num_shots:int|list):\n",
    "        self._num_qubits = num_qubits\n",
    "        self._num_shots = num_shots\n",
    "        self._dm = None\n",
    "        self._entangled = None\n",
    "        self._params = dict()\n",
    "        \n",
    "    @property\n",
    "    def dm(self):\n",
    "        return self._dm\n",
    "    \n",
    "    @dm.setter\n",
    "    def dm(self, new_dm):\n",
    "        if not is_legal(new_dm):\n",
    "            raise ValueError(\"density matrix is not physical\")\n",
    "        else:\n",
    "            self._dm = new_dm\n",
    "            \n",
    "    @dm.deleter\n",
    "    def dm(self):\n",
    "        del self._dm\n",
    "        \n",
    "    @property\n",
    "    def purity(self):\n",
    "        return get_purity(self.dm)\n",
    "    \n",
    "    @property\n",
    "    def TMI(self):\n",
    "        return get_TMI(self.dm)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self._params\n",
    "    \n",
    "    # @params.setter\n",
    "    # def params(self, mode, **kwargs):\n",
    "    #     assert mode in ['Classical Shadow', 'Compressed Sensing', 'QST'], 'allowed modes are: Classical Shadow, Compressed Sensing, QST'\n",
    "    #     if mode == 'Classical Shadow':\n",
    "    #         assert all([k in kwargs.keys() for k in ['target_func', 'sub_mode', 'batch_size']]), 'Classical Shadow method lacks necessary parameters: target_func, sub_mode, batch_size'\n",
    "    #         self._params['Classical Shadow'] = kwargs\n",
    "    #     if mode == 'Compressed Sensing':\n",
    "    #         assert all([k in kwargs.keys() for k in ['target_func', 'num_bases', 'sub_mode']]), 'Compressed Sensing method lacks necessary parameters: target_func, num_bases, sub_mode'\n",
    "    #         self._params['Compressed Sensing'] = kwargs\n",
    "    \n",
    "    def set_params(self, mode, **kwargs):\n",
    "        assert mode in ['Classical Shadow', 'Compressed Sensing', 'Quantum State Tomography'], 'allowed modes are: Classical Shadow, Compressed Sensing, quantum_state_tomography'\n",
    "        if mode == 'Classical Shadow':\n",
    "            required_keys = ['target_func', 'sub_mode', 'batch_size']\n",
    "            assert all(k in kwargs for k in required_keys), f'Classical Shadow method lacks necessary parameters: {\", \".join(required_keys)}'\n",
    "            self._params['Classical Shadow'] = kwargs\n",
    "        elif mode == 'Compressed Sensing':\n",
    "            required_keys = ['target_func', 'num_bases', 'sub_mode']\n",
    "            assert all(k in kwargs for k in required_keys), f'Compressed Sensing method lacks necessary parameters: {\", \".join(required_keys)}'\n",
    "            self._params['Compressed Sensing'] = kwargs\n",
    "        elif mode == 'Quantum State Tomography':\n",
    "            required_keys = ['target_func']\n",
    "            assert all(k in kwargs for k in required_keys), f'Quantum State Tomography method lacks necessary parameters: {\", \".join(required_keys)}'\n",
    "            self._params['Quantum State Tomography'] = kwargs\n",
    "                    \n",
    "    def compressed_sensing(self):\n",
    "        assert 'Compressed Sensing' in self._params.keys(), 'parameters for Compressed Sensing have not been set'\n",
    "        num_bases = self._params['Compressed Sensing']['num_bases']\n",
    "        target_func = self._params['Compressed Sensing']['target_func']\n",
    "        observables = generate_random_Pauli_strings(num_bases, self._num_qubits, pattern='only_XYZ')\n",
    "        expectations = broadcast_Pauli_expectations(self.dm, self._num_qubits, observables, self._num_shots)\n",
    "        \n",
    "        def optimize(dim, expct, tol=1e-5):\n",
    "            sigma = cp.Variable((dim, dim), complex=True)\n",
    "            objective = cp.Minimize(cp.abs(5 * cp.norm(sigma, 'nuc') + 0 * cp.norm(sigma, 'fro') ** 2))\n",
    "            constraints = [cp.trace(sigma) == 1]\n",
    "            for o, e in expct.items():\n",
    "                constraints.append(cp.abs(cp.trace(sigma @ Pauli(o).to_matrix()) - e) <= tol)\n",
    "            problem = cp.Problem(objective, constraints)\n",
    "            problem.solve()\n",
    "            if problem.status not in [cp.OPTIMAL, cp.OPTIMAL_INACCURATE]:\n",
    "                raise ValueError(f\"Optimization failed with given observables\")\n",
    "            return sigma.value\n",
    "        \n",
    "        tolerance = 2 * (self._num_shots / num_bases) ** (-.5)\n",
    "        sigma = optimize(2 ** self._num_qubits, expectations, tol = tolerance)\n",
    "        sigma = regularize(sigma)\n",
    "        return target_func(sigma)\n",
    "    \n",
    "    def quantum_state_tomography(self):\n",
    "        assert 'Quantum State Tomography' in self._params.keys(), 'parameters for Quantum State Tomography have not been set'\n",
    "        target_func = self._params['Quantum State Tomography']['target_func']\n",
    "        all_bases = [''.join(obsv) for obsv in list(itertools.product(*['XYZ' for _ in range(self._num_qubits)]))]\n",
    "        all_expectations = broadcast_Pauli_expectations(self.dm, self._num_qubits, all_bases, self._num_shots)\n",
    "        dm = sum([Pauli(obsv).to_matrix() * expct for obsv, expct in all_expectations.items()]) / 2 ** self._num_qubits\n",
    "        return target_func(dm)\n",
    "    \n",
    "    def random_evolve(self, sub_mode):\n",
    "        if sub_mode == 'Clifford':\n",
    "            self._U = random_clifford(self._num_qubits).to_matrix()\n",
    "            self.dm = self._U @ self.dm @ np.conj(self._U).T\n",
    "        elif sub_mode == 'Pauli':\n",
    "            self._U = [random_clifford(1).to_matrix() for _ in range(self._num_qubits)]\n",
    "            self.dm = tensor_prod(*self._U) @ self.dm @ np.conj(tensor_prod(*self._U)).T\n",
    "    \n",
    "    def single_shot_measure(self, sub_mode):\n",
    "        prob_list = [self._dm[i, i] for i in range(2 ** self._num_qubits)]\n",
    "        single_shot_state = int_to_bin_list(single_sample(prob_list), 2 ** self._num_qubits)\n",
    "        del self._dm\n",
    "        if sub_mode == 'Clifford':\n",
    "            self._state = single_shot_state\n",
    "        elif sub_mode == 'Pauli':\n",
    "            self._state = expand_to_tensor_product(single_shot_state)\n",
    "    \n",
    "    def reconstruct_dm(self, sub_mode):\n",
    "        dim = 2 ** self._num_qubits\n",
    "        if sub_mode == 'Clifford':\n",
    "            return (dim + 1) * (np.conj(self._U).T @ np.outer(self._state, self._state) @ self._U) - np.eye(dim)\n",
    "        elif sub_mode == 'Pauli':\n",
    "            return tensor_prod(*[3 * (np.conj(single_U).T @ np.outer(single_state, single_state) @ single_U) - np.eye(2) \n",
    "                                 for single_U, single_state in zip(self._U, self._state)])\n",
    "    \n",
    "    def classical_shadow(self):\n",
    "        assert 'Classical Shadow' in self._params.keys(), 'parameters for Classical Shadow have not been set'\n",
    "        sub_mode = self._params['Classical Shadow']['sub_mode']\n",
    "        target_func = self._params['Classical Shadow']['target_func']\n",
    "        batch_sizes = self._params['Classical Shadow']['batch_size']\n",
    "        assert sub_mode in ['Clifford', 'Pauli'], 'sub_mode can only be set to Clifford or Pauli'\n",
    "        real_value = target_func(self.dm) # only used to specify data type of the target function\n",
    "        if isinstance(real_value, (float, np.float16, np.float32, np.float64)):\n",
    "            results, result_type = [], 'scalar'\n",
    "        elif isinstance(real_value, dict):\n",
    "            results, result_type = {key: [] for key in real_value.keys()}, 'dict'\n",
    "        dm_copy = self.dm.copy()        \n",
    "        \n",
    "        if isinstance(batch_sizes, (list, np.ndarray)):\n",
    "            if isinstance(batch_sizes, np.ndarray):\n",
    "                batch_sizes = batch_sizes.tolist()\n",
    "                \n",
    "            def get_gcd(a, b):\n",
    "                while b:\n",
    "                    a, b = b, a % b\n",
    "                return a\n",
    "            def get_gcd_multiple(*numbers):\n",
    "                return functools.reduce(get_gcd, numbers)\n",
    "            \n",
    "            batch_gcd = get_gcd_multiple(*batch_sizes)\n",
    "            reduced_sizes = np.array(batch_sizes) // batch_gcd\n",
    "            long_results = copy.deepcopy(results)\n",
    "            all_results = {bs: copy.deepcopy(results) for bs in batch_sizes}\n",
    "            \n",
    "            for _ in range(self._num_shots // batch_gcd):\n",
    "                snapshots = []\n",
    "                for _ in range(batch_sizes):\n",
    "                    self._dm = dm_copy.copy()\n",
    "                    self.random_evolve(sub_mode)\n",
    "                    self.single_shot_measure(sub_mode)\n",
    "                    snapshots.append(self.reconstruct_dm(sub_mode))\n",
    "                mean_dm = np.mean(np.stack(snapshots), axis=0)\n",
    "                sample = target_func(mean_dm)\n",
    "                if result_type == 'scalar':\n",
    "                    long_results.append(sample)\n",
    "                elif result_type == 'dict':\n",
    "                    long_results = {key: value + [sample[key]] for key, value in long_results.items()}\n",
    "            if result_type == 'scalar':\n",
    "                long_results /= np.sum(long_results)\n",
    "            elif result_type == 'dict':\n",
    "                long_results = {key: np.mean(value) for key, value in long_results.items()}\n",
    "            \n",
    "            def get_means(lst, n):\n",
    "                means = []\n",
    "                for i in range(0, len(lst), n):\n",
    "                    chunk = lst[i : i + n]\n",
    "                    chunk_mean = np.sum(chunk) / n\n",
    "                    means.append(chunk_mean)\n",
    "                return means\n",
    "            \n",
    "            for reduced_size in reduced_sizes:\n",
    "                if result_type == 'scalar':\n",
    "                    all_results[reduced_size * batch_gcd] = np.median(get_means(long_results, reduced_size))\n",
    "                elif result_type == 'dict':\n",
    "                    all_results[reduced_size * batch_gcd] = {key: np.median(get_means(long_results[key], reduced_size)) for key in long_results.keys()}\n",
    "            return all_results\n",
    "                                \n",
    "        else: # one single batch_size\n",
    "            for _ in range(self._num_shots // batch_sizes):\n",
    "                snapshots = []\n",
    "                for _ in range(batch_sizes):\n",
    "                    self._dm = dm_copy.copy()\n",
    "                    self.random_evolve(sub_mode)\n",
    "                    self.single_shot_measure(sub_mode)\n",
    "                    snapshots.append(self.reconstruct_dm(sub_mode))\n",
    "                mean_dm = np.mean(np.stack(snapshots), axis=0)\n",
    "                sample = target_func(mean_dm)\n",
    "                if result_type == 'scalar':\n",
    "                    results.append(sample)\n",
    "                elif result_type == 'dict':\n",
    "                    results = {key: value + [sample[key]] for key, value in results.items()}\n",
    "            if result_type == 'scalar':\n",
    "                results = np.median(results)\n",
    "            elif result_type == 'dict':\n",
    "                results = {key: np.median(value) for key, value in results.items()}\n",
    "            return results\n",
    "        \n",
    "    def quantum_state_tomography_for_purity(self): # deprecated\n",
    "        warnings.warn(\"this method has been deprecated\", DeprecationWarning)\n",
    "        max_shots = np.max(self._num_shots)\n",
    "        max_repetition = max_shots // (4 ** self._num_qubits)\n",
    "        all_observables = [''.join(obsv) for obsv in list(itertools.product(*['IXYZ' for _ in range(self._num_qubits)]))]\n",
    "        \n",
    "        def pm1_sample(expct, num_samples):\n",
    "            prob_p1 = (1 + expct) / 2\n",
    "            return [+1 if np.random.random() < prob_p1 else -1 for _ in range(num_samples)]\n",
    "        \n",
    "        all_samples = dict()\n",
    "        for obsv in all_observables:\n",
    "            all_samples[obsv] = pm1_sample(np.trace(self._dm @ Pauli(obsv).to_matrix()).real, max_repetition)\n",
    "        purities = []\n",
    "        for num_shots in self._num_shots:\n",
    "            repetition = num_shots // (4 ** self._num_qubits)\n",
    "            temp_samples = {k: v[:repetition] for k, v in all_samples.items()}\n",
    "            estm_dm = np.sum(np.stack([np.mean(new_v) * Pauli(k).to_matrix() for k, new_v in temp_samples.items()]), axis=0) / (2 ** self._num_qubits)\n",
    "            purities.append(np.trace(estm_dm @ estm_dm).real)\n",
    "        return purities\n",
    "    \n",
    "    def classical_shadow_multi_batches(self): # deprecated\n",
    "        warnings.warn(\"this method has been deprecated\", DeprecationWarning)\n",
    "        assert self._meas in ['Clifford', 'Pauli'], 'only Clifford and Pauli pattern have classical_shadow method'\n",
    "        assert isinstance(self._batch_size, list) or isinstance(self._batch_size, np.ndarray), 'there must be more than one batch size'\n",
    "        if not self._compute_purity:\n",
    "            all_shadows = [{obs: [] for obs in self._observables} for _ in self._batch_size]\n",
    "            dm_copy = self._dm\n",
    "            snapshots = []\n",
    "            for _ in range(self._num_shots):\n",
    "                self._dm = dm_copy\n",
    "                self.random_evolve()\n",
    "                self.single_shot_measure()\n",
    "                snapshots.append(self.reconstruct_dm())\n",
    "            for index, size in enumerate(self._batch_size):\n",
    "                snapshots = split_and_calculate_mean(snapshots, size)\n",
    "                for k in self._observables:\n",
    "                    samples = [np.trace(snapshot @ Pauli(k).to_matrix()).real for snapshot in snapshots]\n",
    "                    all_shadows[index][k] = np.median(samples)\n",
    "            return all_shadows\n",
    "    \n",
    "    def direct_sample(self): # deprecated\n",
    "        warnings.warn(\"this method has been deprecated\", DeprecationWarning)\n",
    "        assert self._meas == 'direct', 'only direct pattern have direct_sample method'\n",
    "        all_samples = {obs: [] for obs in self._observables}\n",
    "        repetition = self._num_shots // len(self._observables)\n",
    "        for k in all_samples.keys():\n",
    "            expct = np.trace(Pauli(k).to_matrix() @ self._dm).real\n",
    "            prob_p1 = (1 + expct) / 2\n",
    "            prob_m1 = (1 - expct) / 2\n",
    "            all_samples[k] = [+1 if np.random.random() < prob_p1 else -1 for _ in range(repetition)]\n",
    "            all_samples[k] = np.mean(all_samples[k])\n",
    "        return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(i_idx, j_idx):\n",
    "#     os.chdir(r'C:\\Users\\Neville\\Documents\\Quantum Computing 3\\Entanglement Detection\\von Neumann Entropy')\n",
    "#     name = str(i_idx) + '-' + str(j_idx) + '.txt'\n",
    "#     path = r'output\\resources01'\n",
    "#     with open(os.path.join(path, name), 'r') as file:\n",
    "#         data = file.read()\n",
    "#     data = data.replace('(', '').replace(')', '').replace('j', 'j ')\n",
    "#     complex_numbers = data.split()\n",
    "#     complex_array = np.array(complex_numbers, dtype=np.complex128)\n",
    "#     sample_dm = complex_array.reshape((8, 8))\n",
    "#     return str(i_idx) + '-' + str(j_idx), sample_dm\n",
    "\n",
    "# i, j, filtered_dms = 0, 0, dict()\n",
    "# while i < 6:\n",
    "#     if j > 150:\n",
    "#         i += 1\n",
    "#         j = 0\n",
    "#     try: \n",
    "#         name, dm = parse(i, j)\n",
    "#         if np.abs(get_TMI(dm)) > 1e-3:\n",
    "#             filtered_dms[name] = dm\n",
    "#         j += 1\n",
    "#     except FileNotFoundError:\n",
    "#         j += 1\n",
    "\n",
    "with open(r'output/resources01.pkl', 'rb') as file:\n",
    "    # Load the data from the file\n",
    "    os.chdir(r'C:\\Users\\Neville\\Documents\\Quantum Computing 3\\Entanglement Detection\\von Neumann Entropy')\n",
    "    filtered_dms = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samples = np.logspace(4, 7, num=4, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "nums_samples = [int(n) for n in nums_samples]\n",
    "all_data_cs = dict()\n",
    "for name, dm in filtered_dms.items():\n",
    "    all_data_cs[name] = [[] for _ in range(len(nums_samples))]\n",
    "    for i in range(len(nums_samples)):\n",
    "        for _ in range(4):\n",
    "            num_samples = nums_samples[i]\n",
    "            qstate = QuantumState(3, num_samples)\n",
    "            qstate.dm = dm\n",
    "            qstate.set_params('Compressed Sensing', num_bases=14, sub_mode='', target_func=get_TMI)   \n",
    "            try:\n",
    "                cs_tmi = qstate.compressed_sensing()\n",
    "            except:\n",
    "                cs_tmi = np.nan\n",
    "            all_data_cs[name][i].append(cs_tmi)\n",
    "        all_data_cs[name][i] = np.array(all_data_cs[name][i])\n",
    "with open (r'output/tmis02-cs.pkl', 'wb') as file:\n",
    "    pickle.dump(all_data_cs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samples = np.logspace(4, 7, num=4, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "nums_samples = [int(n) for n in nums_samples]\n",
    "all_data_qst = dict()\n",
    "for name, dm in filtered_dms.items():\n",
    "    all_data_qst[name] = [[] for _ in range(len(nums_samples))]\n",
    "    for i in range(len(nums_samples)):\n",
    "        for _ in range(4):\n",
    "            num_samples = nums_samples[i]\n",
    "            qstate = QuantumState(3, num_samples)\n",
    "            qstate.dm = dm\n",
    "            qstate.set_params('Quantum State Tomography', target_func=get_TMI)  \n",
    "            try:\n",
    "                qst_tmi = qstate.quantum_state_tomography()\n",
    "            except:\n",
    "                qst_tmi = np.nan\n",
    "            all_data_qst[name][i].append(qst_tmi)\n",
    "        all_data_qst[name][i] = np.array(all_data_qst[name][i])\n",
    "with open (r'output/tmis02-qst.pkl', 'wb') as file:\n",
    "    pickle.dump(all_data_qst, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_names = set(list(filtered_dms.keys()))\n",
    "completed_names = set()\n",
    "all_data_csp = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_samples = np.logspace(4, 5, num=2, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "nums_samples = [int(n) for n in nums_samples]\n",
    "try:\n",
    "    for name, dm in filtered_dms.items():\n",
    "        if name in remaining_names:\n",
    "            all_data_csp[name] = [[] for _ in range(len(nums_samples))]\n",
    "            for i in range(len(nums_samples)):\n",
    "                for _ in range(4):\n",
    "                    num_samples = nums_samples[i]\n",
    "                    qstate = QuantumState(3, num_samples)\n",
    "                    qstate.dm = dm\n",
    "                    qstate.set_params('Classical Shadow', target_func=get_TMI, sub_mode='Pauli', batch_size=int(5e3))  \n",
    "                    # try:\n",
    "                    qst_tmi = qstate.classical_shadow()\n",
    "                    # except:\n",
    "                    #     qst_tmi = np.nan\n",
    "                    all_data_csp[name][i].append(qst_tmi)\n",
    "                all_data_csp[name][i] = np.array(all_data_csp[name][i])\n",
    "            remaining_names.remove(name)\n",
    "            completed_names.add(name)\n",
    "            print(f\"name: {name}'s computation finished\")\n",
    "except:\n",
    "    winsound.Beep(5000, 5000)\n",
    "# with open (r'output/tmis02-qst.pkl', 'wb') as file:\n",
    "#     pickle.dump(all_data_qst, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_Pauli_expectations(dm, num_qubits):\n",
    "    all_Pauli = [''.join(obsv) for obsv in list(itertools.product(*['XYZ' for _ in range(num_qubits)]))]\n",
    "    # all_Pauli.remove(''.join(['I' for _ in range(num_qubits)]))\n",
    "    all_expectations = [np.trace(dm @ Pauli(obsv).to_matrix()).real for obsv in all_Pauli]\n",
    "    return np.mean(np.abs(all_expectations))\n",
    "\n",
    "def plot(name, qst=None, cs=None, csp=None, show=True, save=False):\n",
    "    dm = filtered_dms[name]\n",
    "    true_tmi = get_TMI(dm)\n",
    "    purity = get_purity(dm)\n",
    "    avg_Pauli = get_avg_Pauli_expectations(dm, 3)\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots()\n",
    "    xdata = np.logspace(4, 7, num=4, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "    ax.plot(xdata, [true_tmi] * len(xdata), linestyle='--', c='black', label='theoretical')\n",
    "    optm = np.inf\n",
    "    if qst:\n",
    "        xdata = np.logspace(4, 7, num=4, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "        mean_data = [np.mean(series) for series in all_data_qst[name]]\n",
    "        std_dev_data = [np.std(series) for series in all_data_qst[name]]\n",
    "        ax.plot(xdata, mean_data, c='purple', linewidth=2, label='Quantum State Tomography')\n",
    "        ax.errorbar(xdata, mean_data, yerr=std_dev_data, fmt='o', markersize=4, color='purple' , ecolor='purple', capsize=1)\n",
    "        rel_errors = (np.array(mean_data) - true_tmi) / true_tmi\n",
    "        best = rel_errors[np.argmin(np.abs(rel_errors))]\n",
    "        if np.abs(best) < np.abs(optm):\n",
    "            optm = best\n",
    "    if cs:\n",
    "        xdata = np.logspace(4, 7, num=4, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "        mean_data = [np.mean(series) for series in all_data_cs[name]]\n",
    "        std_dev_data = [np.std(series) for series in all_data_cs[name]]\n",
    "        # ax.plot(xdata, mean_data, c='green', linewidth=2, label='Compressed Sensing')\n",
    "        ax.errorbar(xdata, mean_data, yerr=std_dev_data, fmt='o', markersize=4, color='green', ecolor='green', capsize=1)\n",
    "        # for i in range(5):\n",
    "        #     temp_data = [series[i] for series in all_data_cs[name]]\n",
    "        #     ax.scatter(xdata, temp_data)\n",
    "        rel_errors = (np.array(mean_data) - true_tmi) / true_tmi\n",
    "        best = rel_errors[np.argmin(np.abs(rel_errors))]\n",
    "        if np.abs(best) < np.abs(optm):\n",
    "            optm = best\n",
    "    if csp:\n",
    "        xdata = np.logspace(4, 5, num=2, endpoint=True, base=10.0, dtype=None, axis=0)\n",
    "        mean_data = [np.mean(series) for series in all_data_csp[name]]\n",
    "        std_dev_data = [np.std(series) for series in all_data_csp[name]]\n",
    "        # ax.plot(xdata, mean_data, c='blue', linewidth=2, label='Classical Shadow Pauli')\n",
    "        ax.errorbar(xdata, mean_data, yerr=std_dev_data, fmt='o', markersize=4, color='blue', ecolor='blue', capsize=1)\n",
    "        rel_errors = (np.array(mean_data) - true_tmi) / true_tmi\n",
    "        best = rel_errors[np.argmin(np.abs(rel_errors))]\n",
    "        if np.abs(best) < np.abs(optm):\n",
    "            optm = best\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel(r'$TMI$')\n",
    "    ax.set_xlabel(r'number of shots')\n",
    "    ax.set_title(f'state {name}: purity={purity:.2f}, avg={avg_Pauli:.2f}, optm={optm*100:.2f}% \\n TMI={true_tmi:4f} (each point repeated five times)')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(r'figs\\hybrid', name + '.png'), dpi=300)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in filtered_dms:\n",
    "#     plot(name, cs=True, qst=True, csp=False, show=False, save=True)\n",
    "plot('0-0', cs=True, qst=True, csp=False, show=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purities = {'5-6-1': [], '5-6-2': [], '6-7-1': [], '6-7-2': [], '7-8-1': [], '7-8-2': [], '8-9-1': [], '8-9-2': [], '9-0-1': [],  '9-0-2': []}\n",
    "for name, series in all_data_cs.items():\n",
    "    dm = filtered_dms[name]\n",
    "    true_tmi = get_TMI(dm)\n",
    "    purity = get_purity(dm)\n",
    "    series = series[3]\n",
    "    rel_errs = (series - true_tmi) / true_tmi\n",
    "    best = np.mean(rel_errs)\n",
    "    if .5 < purity < .55:\n",
    "        purities['5-6-1'].append(best)\n",
    "    if .55 < purity < .6:\n",
    "        purities['5-6-2'].append(best)        \n",
    "    if .6 < purity < .65:\n",
    "        purities['6-7-1'].append(best)  \n",
    "    if .65 < purity < .7:\n",
    "        purities['6-7-2'].append(best)  \n",
    "    if .7 < purity < .75:\n",
    "        purities['7-8-1'].append(best) \n",
    "    if .75 < purity < .8:\n",
    "        purities['7-8-2'].append(best)         \n",
    "    if .8 < purity < .85:\n",
    "        purities['8-9-1'].append(best) \n",
    "    if .85 < purity < .9:\n",
    "        purities['8-9-2'].append(best) \n",
    "    if .9 < purity < .95:\n",
    "        purities['9-0-1'].append(best) \n",
    "    if .95 < purity < 1:\n",
    "        purities['9-0-2'].append(best) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(5, 10, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = np.arange(5, 10, .5)\n",
    "data = [np.abs(np.mean(np.abs(values))) for values in purities.values()]\n",
    "plt.plot(xdata, data)\n",
    "plt.scatter(xdata, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
